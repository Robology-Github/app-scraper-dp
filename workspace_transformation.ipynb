{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords already installed.\n",
      "Empty DataFrame\n",
      "Columns: [Language, Number of Apps]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [Genre, Number of Apps]\n",
      "Index: []\n",
      "  Device Type  Number of Apps\n",
      "0      iPhone              50\n",
      "1        iPad              50\n",
      "2         Mac               1\n",
      "Bigrams and word frequencies have been saved to CSV files.\n",
      "                         appId                           title  \\\n",
      "0          se.memfrag.VeniVidi                            been   \n",
      "1  com.travelboast.travelboast  TravelBoast: My Journey Routes   \n",
      "2   com.tripadvisor.LocalPicks  Tripadvisor: Plan & Book Trips   \n",
      "3      com.HighHeels.Travelist  Visited: My Travel Map & Lists   \n",
      "4       com.booking.BookingApp        Booking.com Travel Deals   \n",
      "\n",
      "                                                 url   primaryGenre  \\\n",
      "0  https://apps.apple.com/cz/app/been/id680148327...         Travel   \n",
      "1  https://apps.apple.com/cz/app/travelboast-my-j...  Photo & Video   \n",
      "2  https://apps.apple.com/cz/app/tripadvisor-plan...         Travel   \n",
      "3  https://apps.apple.com/cz/app/visited-my-trave...         Travel   \n",
      "4  https://apps.apple.com/cz/app/booking-com-trav...         Travel   \n",
      "\n",
      "  contentRating       size                  released  \\\n",
      "0            4+   68823040 2013-07-30 23:28:37+00:00   \n",
      "1            4+  246719488 2019-09-03 07:00:00+00:00   \n",
      "2            4+  171645952 2019-02-05 08:00:00+00:00   \n",
      "3            4+  168248320 2014-04-08 15:06:29+00:00   \n",
      "4            4+  268875776 2010-04-14 18:30:24+00:00   \n",
      "\n",
      "                    updated  price  free  ... days_since_last_update app_age  \\\n",
      "0 2023-09-24 16:15:58+00:00      0  free  ...                    203    3707   \n",
      "1 2024-04-06 01:35:01+00:00      0  free  ...                      8    1676   \n",
      "2 2024-04-10 08:48:18+00:00      0  free  ...                      4    1891   \n",
      "3 2023-12-18 09:51:48+00:00      0  free  ...                    118    3540   \n",
      "4 2024-04-11 10:37:31+00:00      0  free  ...                      3    5110   \n",
      "\n",
      "                                   processed_reviews  \\\n",
      "0  good app simple premium account cheap hi exper...   \n",
      "1  cool app good need year usedli app make youtub...   \n",
      "2  app good easily udnerstandable reliable conten...   \n",
      "3  application requires consent send advertising ...   \n",
      "4  everything goes smoothly great screwed custome...   \n",
      "\n",
      "                                             bigrams  \\\n",
      "0  [good app, app simple, simple premium, premium...   \n",
      "1  [cool app, app good, good need, need year, yea...   \n",
      "2  [app good, good easily, easily udnerstandable,...   \n",
      "3  [application requires, requires consent, conse...   \n",
      "4  [everything goes, goes smoothly, smoothly grea...   \n",
      "\n",
      "                                           word_freq  Sentiment_Category  \\\n",
      "0  [(good, 2), (app, 10), (simple, 2), (premium, ...   Slightly positive   \n",
      "1  [(cool, 1), (app, 3), (good, 2), (need, 1), (y...   Slightly positive   \n",
      "2  [(app, 13), (good, 3), (easily, 1), (udnerstan...             Neutral   \n",
      "3  [(application, 2), (requires, 1), (consent, 3)...            Negative   \n",
      "4  [(everything, 1), (goes, 1), (smoothly, 1), (g...   Slightly negative   \n",
      "\n",
      "      update_frequency  price_category  app_age_category  file_size_category  \n",
      "0       Rarely Updated            Free       Very Mature              Medium  \n",
      "1  Very Recent Updates            Free       Very Mature               Large  \n",
      "2  Very Recent Updates            Free       Very Mature              Medium  \n",
      "3   Moderately Updated            Free       Very Mature              Medium  \n",
      "4  Very Recent Updates            Free       Very Mature               Large  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from iso639 import languages\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('./AppStoreOutput.csv', delimiter=',', encoding='utf-8')\n",
    "df['released'] = pd.to_datetime(df['released'])\n",
    "df['updated'] = pd.to_datetime(df['updated'])\n",
    "df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "df['free'] = df['free'].astype(int)\n",
    "df['supports_iPhone'] = 0\n",
    "df['supports_iPad'] = 0\n",
    "df['supports_Mac'] = 0\n",
    "df['days_since_last_update'] = (datetime.now(timezone.utc) - df['updated']).dt.days\n",
    "df['app_age'] = (df['updated'] - df['released']).dt.days\n",
    "df['reviews'] = df['reviews'].astype(str)\n",
    "\n",
    "# Load the sentiment analysis pipeline with the multilingual BERT model\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "\n",
    "def ensure_stopwords():\n",
    "    # Define the default path (adjust as needed for your system)\n",
    "    default_path = os.path.join(nltk.data.path[0], 'corpora', 'stopwords')\n",
    "    if not os.path.exists(default_path):\n",
    "        print(\"Downloading NLTK stopwords...\")\n",
    "        nltk.download('stopwords')\n",
    "    else:\n",
    "        print(\"Stopwords already installed.\")\n",
    "\n",
    "ensure_stopwords()\n",
    "\n",
    "\n",
    "# Mapping from language names to NLTK compatible language codes\n",
    "nltk_lang_map = {\n",
    "    'ar': 'arabic',\n",
    "    'az': 'azerbaijani',\n",
    "    'eu': 'basque',\n",
    "    'bn': 'bengali',\n",
    "    'ca': 'catalan',\n",
    "    'zh': 'chinese',\n",
    "    'da': 'danish',\n",
    "    'nl': 'dutch',\n",
    "    'en': 'english',\n",
    "    'fi': 'finnish',\n",
    "    'fr': 'french',\n",
    "    'de': 'german',\n",
    "    'el': 'greek',\n",
    "    'he': 'hebrew',\n",
    "    'hu': 'hungarian',\n",
    "    'id': 'indonesian',\n",
    "    'it': 'italian',\n",
    "    'kk': None,  # No support in NLTK\n",
    "    'ne': None,  # No support in NLTK\n",
    "    'no': 'norwegian',\n",
    "    'pt': 'portuguese',\n",
    "    'ro': 'romanian',\n",
    "    'ru': 'russian',\n",
    "    'sl': 'slovene',\n",
    "    'es': 'spanish',\n",
    "    'sv': 'swedish',\n",
    "    'tg': None,  # No support in NLTK\n",
    "    'tr': 'turkish'\n",
    "}\n",
    "\n",
    "def get_stopwords(text):\n",
    "    try:\n",
    "        # Detect the language of the text\n",
    "        lang = detect(text)\n",
    "        # Get the stopwords for the detected language\n",
    "        stopwords_lang = nltk_lang_map.get(lang, 'english')\n",
    "        if stopwords_lang:\n",
    "            return set(stopwords.words(stopwords_lang))\n",
    "        else:\n",
    "            return set(stopwords.words('english'))\n",
    "    except Exception as e:\n",
    "        print(\"Error in detecting language or loading stopwords:\", e)\n",
    "        return set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_and_split_reviews(reviews):\n",
    "    # Convert reviews to string to avoid TypeError with non-string inputs\n",
    "    if pd.isna(reviews):\n",
    "        return \"\"  # Return an empty string if the review is NaN\n",
    "    reviews = str(reviews)\n",
    "    try:\n",
    "        # Use language-specific stopwords\n",
    "        stop_words = get_stopwords(reviews)\n",
    "    except Exception as e:\n",
    "        print(\"Error using language-specific stopwords:\", e)\n",
    "        stop_words = set(stopwords.words('english'))  # Default to English if error occurs\n",
    "\n",
    "    # Remove all non-alpha characters and extra spaces, convert to lower case\n",
    "    reviews = re.sub('[^\\wáčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ]', ' ', reviews, flags=re.UNICODE)\n",
    "    reviews = re.sub('\\s+', ' ', reviews).strip().lower()\n",
    "    # Remove stopwords\n",
    "    words = [word for word in reviews.split() if word not in stop_words and len(word) > 1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the modified function to your DataFrame\n",
    "df['processed_reviews'] = df['reviews'].apply(preprocess_and_split_reviews)\n",
    "\n",
    "def get_and_flatten_bigrams(text):\n",
    "    if len(text.split()) < 2:\n",
    "        return []\n",
    "    blob = TextBlob(text)\n",
    "    return [' '.join(bigram) for bigram in blob.ngrams(2)]\n",
    "\n",
    "df['bigrams'] = df['processed_reviews'].apply(get_and_flatten_bigrams)\n",
    "bigrams_df = df.explode('bigrams')[['appId', 'bigrams']].dropna()\n",
    "\n",
    "def flatten_word_frequencies(text):\n",
    "    freqs = Counter(text.split())\n",
    "    return list(freqs.items())\n",
    "\n",
    "df['word_freq'] = df['processed_reviews'].apply(flatten_word_frequencies)\n",
    "word_freq_rows = df.explode('word_freq')\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'appId': word_freq_rows['appId'],\n",
    "    'word': word_freq_rows['word_freq'].apply(lambda x: x[0] if pd.notna(x) else ''),\n",
    "    'frequency': word_freq_rows['word_freq'].apply(lambda x: x[1] if pd.notna(x) else 0)\n",
    "}).dropna()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if 'iPhone' in row['supportedDevices']:\n",
    "        df.at[index, 'supports_iPhone'] = 1\n",
    "    if 'iPad' in row['supportedDevices']:\n",
    "        df.at[index, 'supports_iPad'] = 1\n",
    "    if 'Mac' in row['supportedDevices']:\n",
    "        df.at[index, 'supports_Mac'] = 1\n",
    "\n",
    "\n",
    "\n",
    "def compute_sentiment_category_mbert(text):\n",
    "    try:\n",
    "        # Directly pass the text to the pipeline\n",
    "        # The pipeline handles tokenization and truncation internally\n",
    "        result = sentiment_analyzer(text, truncation=True, max_length=512)[0]\n",
    "        label = result['label']\n",
    "        \n",
    "        # Mapping the model output to custom categories\n",
    "        if label == '1 star':\n",
    "            return 'Negative'\n",
    "        elif label == '2 stars':\n",
    "            return 'Slightly negative'\n",
    "        elif label == '3 stars':\n",
    "            return 'Neutral'\n",
    "        elif label == '4 stars':\n",
    "            return 'Slightly positive'\n",
    "        else:  # '5 stars'\n",
    "            return 'Positive'\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return 'Missing'  # Default to 'Missing' in case of an error\n",
    "\n",
    "df['Sentiment_Category'] = df['reviews'].apply(compute_sentiment_category_mbert)\n",
    "\n",
    "\n",
    "# Parsing and One-hot Encoding for List Columns\n",
    "def parse_list_column(column):\n",
    "    try:\n",
    "        return column.apply(literal_eval)\n",
    "    except ValueError:\n",
    "        return column\n",
    "\n",
    "df['languages'] = parse_list_column(df['languages'])\n",
    "df['genres'] = parse_list_column(df['genres'])\n",
    "\n",
    "languages_exploded = df[['appId', 'languages']].explode('languages')\n",
    "genres_exploded = df[['appId', 'genres']].explode('genres')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Expanded Language-to-Countries Mapping\n",
    "language_to_countries = {\n",
    "    'AF': ['Afghanistan'],\n",
    "    'AM': ['Armenia'],\n",
    "    'AN': ['Netherlands Antilles'],\n",
    "    'AR': ['Saudi Arabia', 'Iraq', 'Egypt', 'Algeria', 'Morocco', 'Sudan', 'Yemen', 'Syria', 'Tunisia', 'Jordan', 'Libya', 'Lebanon', 'Oman', 'Kuwait', 'Mauritania', 'Qatar', 'Bahrain', 'United Arab Emirates'],\n",
    "    'AZ': ['Azerbaijan'],\n",
    "    'BE': ['Belarus'],\n",
    "    'BG': ['Bulgaria'],\n",
    "    'BN': ['Bangladesh', 'India'],\n",
    "    'BR': ['Brazil'],\n",
    "    'BS': ['Bosnia and Herzegovina'],\n",
    "    'CA': ['Spain', 'Andorra'],\n",
    "    'CO': ['France'],\n",
    "    'CS': ['Czech Republic', 'Slovakia'],\n",
    "    'CY': ['Wales'],\n",
    "    'DA': ['Denmark', 'Greenland', 'Faroe Islands'],\n",
    "    'DE': ['Germany', 'Austria', 'Switzerland', 'Luxembourg', 'Liechtenstein'],\n",
    "    'EL': ['Greece', 'Cyprus'],\n",
    "    'EN': ['United States', 'United Kingdom', 'Canada', 'Australia', 'Ireland', 'New Zealand', 'South Africa'],\n",
    "    'EO': ['Worldwide'],  # Esperanto is a constructed international auxiliary language.\n",
    "    'ES': ['Spain', 'Mexico', 'Colombia', 'Argentina', 'Peru', 'Venezuela', 'Chile', 'Ecuador', 'Guatemala', 'Cuba', 'Bolivia', 'Dominican Republic', 'Honduras', 'Paraguay', 'El Salvador', 'Nicaragua', 'Costa Rica', 'Puerto Rico', 'Panama', 'Uruguay'],\n",
    "    'ET': ['Estonia'],\n",
    "    'EU': ['Spain'],  # Basque Country\n",
    "    'FA': ['Iran', 'Afghanistan', 'Tajikistan'],\n",
    "    'FI': ['Finland', 'Sweden'],\n",
    "    'FR': ['France', 'Canada', 'Belgium', 'Switzerland', 'Luxembourg', 'Monaco', 'Congo', 'Ivory Coast', 'Madagascar', 'Cameroon', 'Burkina Faso', 'Niger', 'Senegal', 'Mali', 'Rwanda', 'Belgium', 'Guinea'],\n",
    "    'FY': ['Netherlands'],\n",
    "    'GA': ['Ireland'],\n",
    "    'GD': ['Scotland'],\n",
    "    'GL': ['Spain'],\n",
    "    'GU': ['India'],\n",
    "    'HE': ['Israel'],\n",
    "    'HI': ['India'],\n",
    "    'HR': ['Croatia', 'Bosnia and Herzegovina'],\n",
    "    'HT': ['Haiti'],\n",
    "    'HU': ['Hungary'],\n",
    "    'HY': ['Armenia', 'Nagorno-Karabakh Republic'],\n",
    "    'IA': ['Worldwide'],  # Interlingua is a constructed international auxiliary language.\n",
    "    'ID': ['Indonesia'],\n",
    "    'IG': ['Nigeria'],\n",
    "    'IS': ['Iceland'],\n",
    "    'IT': ['Italy', 'Switzerland', 'San Marino', 'Vatican City'],\n",
    "    'JA': ['Japan'],\n",
    "    'KA': ['Georgia'],\n",
    "    'KK': ['Kazakhstan'],\n",
    "    'KM': ['Cambodia'],\n",
    "    'KN': ['India'],\n",
    "    'KO': ['South Korea', 'North Korea'],\n",
    "    'KU': ['Turkey', 'Iraq', 'Iran', 'Syria'],\n",
    "    'KY': ['Kyrgyzstan'],\n",
    "    'LO': ['Laos'],\n",
    "    'LT': ['Lithuania'],\n",
    "    'LV': ['Latvia'],\n",
    "    'MK': ['North Macedonia'],\n",
    "    'ML': ['India', 'Sri Lanka'],\n",
    "    'MN': ['Mongolia'],\n",
    "    'MR': ['India'],\n",
    "    'MS': ['Malaysia', 'Brunei', 'Singapore'],\n",
    "    'MT': ['Malta'],\n",
    "    'MY': ['Myanmar'],\n",
    "    'NB': ['Norway'],\n",
    "    'NE': ['Niger'],\n",
    "    'NL': ['Netherlands', 'Belgium', 'Suriname'],\n",
    "    'NN': ['Norway'],\n",
    "    'OC': ['France'],\n",
    "    'PA': ['India', 'Pakistan'],\n",
    "    'PL': ['Poland'],\n",
    "    'PS': ['Afghanistan', 'Pakistan'],\n",
    "    'PT': ['Portugal', 'Brazil', 'Angola', 'Mozambique', 'Cape Verde', 'Guinea-Bissau', 'São Tomé and Príncipe', 'East Timor'],\n",
    "    'RO': ['Romania', 'Moldova'],\n",
    "    'RU': ['Russia', 'Belarus', 'Kazakhstan', 'Kyrgyzstan'],\n",
    "    'SC': ['Italy'],\n",
    "    'SE': ['Sweden'],\n",
    "    'SI': ['Sri Lanka'],\n",
    "    'SK': ['Slovakia'],\n",
    "    'SL': ['Slovenia'],\n",
    "    'SN': ['Zimbabwe'],\n",
    "    'SQ': ['Albania', 'Kosovo'],\n",
    "    'SR': ['Serbia', 'Bosnia and Herzegovina', 'Montenegro', 'Kosovo'],\n",
    "    'SV': ['Sweden'],\n",
    "    'SW': ['Tanzania', 'Kenya', 'Uganda'],\n",
    "    'TA': ['India', 'Sri Lanka'],\n",
    "    'TE': ['India'],\n",
    "    'TG': ['Tajikistan'],\n",
    "    'TH': ['Thailand'],\n",
    "    'TL': ['Timor-Leste'],\n",
    "    'TR': ['Turkey', 'Cyprus'],\n",
    "    'TT': ['Russia'],\n",
    "    'UK': ['Ukraine'],\n",
    "    'UR': ['Pakistan', 'India'],\n",
    "    'UZ': ['Uzbekistan'],\n",
    "    'VI': ['Vietnam'],\n",
    "    'XH': ['South Africa'],\n",
    "    'YI': ['Worldwide'],  # Yiddish is spoken by Jewish communities worldwide.\n",
    "    'YO': ['Nigeria', 'Benin'],\n",
    "    'ZH': ['China', 'Taiwan', 'Singapore', 'Malaysia'],\n",
    "    'ZU': ['South Africa'],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Map language codes to countries\n",
    "languages_exploded['Countries'] = languages_exploded['languages'].map(lambda x: ', '.join(language_to_countries.get(x, ['Unknown'])))\n",
    "languages_exploded['Countries'] = languages_exploded['Countries'].fillna('Unknown')\n",
    "# Split the 'Countries' column into a list of countries\n",
    "languages_exploded['Countries'] = languages_exploded['Countries'].str.split(', ')\n",
    "# Explode the 'Countries' column\n",
    "languages_exploded = languages_exploded.explode('Countries')\n",
    "\n",
    "def get_language_name(lang_code):\n",
    "    # Convert the language code to lowercase to match the iso639 library's expected format\n",
    "    lang_code_lower = lang_code.lower()\n",
    "    try:\n",
    "        # Attempt to get the language name using the ISO 639-1 code\n",
    "        lang_name = languages.get(part1=lang_code_lower).name\n",
    "    except KeyError:\n",
    "        try:\n",
    "            # If the ISO 639-1 code lookup fails, try the ISO 639-2/T code\n",
    "            lang_name = languages.get(part2t=lang_code_lower).name\n",
    "        except KeyError:\n",
    "            try:\n",
    "                # If the ISO 639-2/T code lookup also fails, try the ISO 639-2/B code\n",
    "                lang_name = languages.get(part2b=lang_code_lower).name\n",
    "            except KeyError:\n",
    "                # If none of the lookups are successful, return the original code\n",
    "                lang_name = lang_code  # Keeping the original case for visibility\n",
    "    return lang_name\n",
    "\n",
    "# Apply the function to translate language codes to names\n",
    "languages_exploded['language_name'] = languages_exploded['languages'].apply(get_language_name)\n",
    "\n",
    "\n",
    "\n",
    "# Categorizing Numerical Data\n",
    "bins = [0, 50000000, 200000000, float('inf')]\n",
    "labels = ['Small', 'Medium', 'Large']\n",
    "\n",
    "\n",
    "# Function to convert binary values to 'free' or 'paid'\n",
    "def free_convert_to_category(value):\n",
    "    if value == 1:\n",
    "        return 'free'\n",
    "    else:\n",
    "        return 'paid'\n",
    "\n",
    "## App age categorization\n",
    "def categorize_app_age(days):\n",
    "    if days <= 30:\n",
    "        return 'Brand New'\n",
    "    elif days <= 90:\n",
    "        return 'Recently Launched'\n",
    "    elif days <= 365:\n",
    "        return 'Established'\n",
    "    elif days <= 1095:\n",
    "        return 'Mature'\n",
    "    else:\n",
    "        return 'Very Mature'\n",
    "    \n",
    "## Price categorization\n",
    "def categorize_price(price):\n",
    "    if price == 0:\n",
    "        return 'Free'\n",
    "    elif price < 1:\n",
    "        return 'Low price'\n",
    "    elif price <= 10:\n",
    "        return 'Medium price'\n",
    "    else:\n",
    "        return 'High price'\n",
    "        \n",
    "\n",
    "## Update frequency categorization\n",
    "def categorize_update_frequency(days_since_last_update):\n",
    "    if days_since_last_update <= 30:\n",
    "        return 'Very Recent Updates'\n",
    "    elif days_since_last_update <= 90:\n",
    "        return 'Recently Updated'\n",
    "    elif days_since_last_update <= 180:\n",
    "        return 'Moderately Updated'\n",
    "    elif days_since_last_update <= 365:\n",
    "        return 'Rarely Updated'\n",
    "    else:\n",
    "        return 'Stale'\n",
    "\n",
    "\n",
    "\n",
    "# Process reviews\n",
    "df['Sentiment_Category'] = df['reviews'].apply(compute_sentiment_category_mbert)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['update_frequency'] = df['days_since_last_update'].apply(categorize_update_frequency)\n",
    "\n",
    "## Apply the categorization function to the 'price' column\n",
    "df['price_category'] = df['price'].apply(lambda price: categorize_price(price))\n",
    "\n",
    "## Apply the categorization function to the 'app_age' column\n",
    "df['app_age_category'] = df['app_age'].apply(lambda days: categorize_app_age(days))\n",
    "\n",
    "## Apply the categorization function to the 'file_size' column\n",
    "df['file_size_category'] = pd.cut(df['size'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# Aggregate Languages and Genres\n",
    "language_counts = df.filter(regex='^lang_').sum().reset_index()\n",
    "genre_counts = df.filter(regex='^_').sum().reset_index()\n",
    "\n",
    "# Remove '_' prefix and rename columns\n",
    "genre_counts['index'] = genre_counts['index'].str.replace('^_', '', regex=True)\n",
    "genre_counts.columns = ['Genre', 'Number of Apps']\n",
    "\n",
    "# Remove 'lang_' prefix and rename columns\n",
    "language_counts['index'] = language_counts['index'].str.replace('lang_', '', regex=True)\n",
    "language_counts.columns = ['Language', 'Number of Apps']\n",
    "\n",
    "\n",
    "# Creating a relational table for device support\n",
    "device_support = df.melt(id_vars=['appId'], value_vars=['supports_iPhone', 'supports_iPad', 'supports_Mac'], var_name='Device', value_name='Supported')\n",
    "device_support = device_support[device_support['Supported'] == 1].drop('Supported', axis=1)\n",
    "device_support.to_csv('AppStore_Device_Support.csv', index=False)\n",
    "\n",
    "\n",
    "# Apply the function to the 'free' column\n",
    "df['free'] = df['free'].apply(free_convert_to_category)\n",
    "\n",
    "\n",
    "# Rename columns for clarity\n",
    "language_counts.columns = ['Language', 'Number of Apps']\n",
    "genre_counts.columns = ['Genre', 'Number of Apps']\n",
    "\n",
    "# Preview the aggregated language data\n",
    "print(language_counts.head())\n",
    "print('\\n')\n",
    "# Preview the aggregated genre data\n",
    "print(genre_counts.head())\n",
    "\n",
    "# Device support aggregation\n",
    "device_support_counts = df[['supports_iPhone', 'supports_iPad', 'supports_Mac']].sum().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "device_support_counts.columns = ['Device Type', 'Number of Apps']\n",
    "\n",
    "# Convert device type names to more readable format if necessary\n",
    "# Example: You can manually rename each type for clarity\n",
    "device_support_counts['Device Type'] = device_support_counts['Device Type'].replace({\n",
    "    'supports_iPhone': 'iPhone',\n",
    "    'supports_iPad': 'iPad',\n",
    "    'supports_Mac': 'Mac'\n",
    "})\n",
    "\n",
    "# Preview the device support data\n",
    "print(device_support_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Final DataFrame Cleanup and Saving the Cleaned Data\n",
    "columns_to_remove = [\n",
    "    'id', '', 'description', 'icon', 'genreIds', 'primaryGenreId',\n",
    "    'requiredOsVersion', 'releaseNotes', 'version', 'developerid', 'developerUrl',\n",
    "     'screenshots', 'ipadScreenshots', 'appletvScreenshots',\n",
    "    'languages', 'genres', 'supportedDevices', 'currency', 'developerId', 'reviews', \n",
    "]\n",
    "df.drop(columns_to_remove, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('AppStoreOutput_cleaned.csv', index=False, sep=',', encoding='utf-8')\n",
    "languages_exploded.to_csv('AppStore_Languages.csv', index=False)\n",
    "genres_exploded.to_csv('AppStore_Genres.csv', index=False)\n",
    "\n",
    "# Save the results to separate CSV files\n",
    "bigrams_df.to_csv('AppStore_Bigrams.csv', index=False)\n",
    "word_freq_df.to_csv('AppStore_Word_Frequencies.csv', index=False)\n",
    "print(\"Bigrams and word frequencies have been saved to CSV files.\")\n",
    "\n",
    "# Preview the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/filipnamdao/nltk_data/corpora/stopwords\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "path = nltk.data.find('corpora/stopwords')\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/filipnamdao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/filipnamdao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           title  minInstalls     score  ratings  price  free  \\\n",
      "0                   Dny Marianne       100000  3.186275      553      0  free   \n",
      "1  Temu: Shop Like a Billionaire    100000000  4.679223  3001024      0  free   \n",
      "2       Allegro: shopping online     10000000  3.914894   454233      0  free   \n",
      "3                      O2 TV 2.0       100000  1.280457     7836      0  free   \n",
      "4                       eDoklady       100000  2.931915     2347      0  free   \n",
      "\n",
      "   available  offersIAP IAPRange                        developer  ...  \\\n",
      "0       True      False      nan         BurdaMedia Extra s r. o.  ...   \n",
      "1       True      False      nan                             Temu  ...   \n",
      "2       True      False      nan               Allegro sp. z o.o.  ...   \n",
      "3       True      False      nan           O2 Czech Republic a.s.  ...   \n",
      "4       True      False      nan  Digitální a informační agentura  ...   \n",
      "\n",
      "  CurrencySymbolMin IAPMin CurrencySymbolMax  IAPMax     update_frequency  \\\n",
      "0               NaN    NaN               NaN     NaN     Recently Updated   \n",
      "1               NaN    NaN               NaN     NaN  Very Recent Updates   \n",
      "2               NaN    NaN               NaN     NaN  Very Recent Updates   \n",
      "3               NaN    NaN               NaN     NaN     Recently Updated   \n",
      "4               NaN    NaN               NaN     NaN  Very Recent Updates   \n",
      "\n",
      "  app_age_category  rating_ratio_category  engagement_score_category  \\\n",
      "0      Very Mature                  Mixed        Very Low Engagement   \n",
      "1           Mature            Exceptional        Moderate Engagement   \n",
      "2      Very Mature                   Good            High Engagement   \n",
      "3           Mature                   Poor        Moderate Engagement   \n",
      "4      Established                   Poor             Low Engagement   \n",
      "\n",
      "   price_category install_to_rating_category  \n",
      "0            Free      Moderate Review Ratio  \n",
      "1            Free          High Review Ratio  \n",
      "2            Free          High Review Ratio  \n",
      "3            Free          High Review Ratio  \n",
      "4            Free          High Review Ratio  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from ast import literal_eval\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Load and transform data\n",
    "df = pd.read_csv('./GooglePlayOutput.csv', delimiter=',', encoding='utf-8')\n",
    "df['released'] = pd.to_datetime(df['released']).dt.tz_localize('UTC')\n",
    "df['updated'] = pd.to_datetime(df['updated'], unit='ms', utc=True)\n",
    "df['days_since_last_update'] = (datetime.now(timezone.utc) - df['updated']).dt.days\n",
    "\n",
    "# Clean data\n",
    "df['contentRating'] = df['contentRating'].str.replace('Rated for', '', regex=False).str.strip()\n",
    "df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
    "df['free'] = df['free'].astype(int)\n",
    "\n",
    "\n",
    "# Ensure the 'IAPRange' column exists and is of string type\n",
    "if 'IAPRange' in df.columns and df['IAPRange'].dtype != object:\n",
    "    df['IAPRange'] = df['IAPRange'].astype(str)\n",
    "\n",
    "# Load the sentiment analysis pipeline with the multilingual BERT model\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Function to ensure stopwords are available\n",
    "def ensure_stopwords():\n",
    "    try:\n",
    "        # Check if stopwords are available\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        # If not available, download stopwords\n",
    "        print(\"Downloading NLTK stopwords...\")\n",
    "        nltk.download('stopwords')\n",
    "    else:\n",
    "        print(\"Stopwords already installed.\")\n",
    "\n",
    "# Ensure stopwords are ready before use\n",
    "ensure_stopwords()\n",
    "\n",
    "\n",
    "# Mapping from language names to NLTK compatible language codes\n",
    "nltk_lang_map = {\n",
    "    'ar': 'arabic',\n",
    "    'az': 'azerbaijani',\n",
    "    'eu': 'basque',\n",
    "    'bn': 'bengali',\n",
    "    'ca': 'catalan',\n",
    "    'zh': 'chinese',\n",
    "    'da': 'danish',\n",
    "    'nl': 'dutch',\n",
    "    'en': 'english',\n",
    "    'fi': 'finnish',\n",
    "    'fr': 'french',\n",
    "    'de': 'german',\n",
    "    'el': 'greek',\n",
    "    'he': 'hebrew',\n",
    "    'hu': 'hungarian',\n",
    "    'id': 'indonesian',\n",
    "    'it': 'italian',\n",
    "    'kk': None,  # No support in NLTK\n",
    "    'ne': None,  # No support in NLTK\n",
    "    'no': 'norwegian',\n",
    "    'pt': 'portuguese',\n",
    "    'ro': 'romanian',\n",
    "    'ru': 'russian',\n",
    "    'sl': 'slovene',\n",
    "    'es': 'spanish',\n",
    "    'sv': 'swedish',\n",
    "    'tg': None,  # No support in NLTK\n",
    "    'tr': 'turkish'\n",
    "}\n",
    "\n",
    "def get_stopwords(text):\n",
    "    try:\n",
    "        # Detect the language of the text\n",
    "        lang = detect(text)\n",
    "        # Get the stopwords for the detected language\n",
    "        stopwords_lang = nltk_lang_map.get(lang, 'english')\n",
    "        if stopwords_lang:\n",
    "            return set(stopwords.words(stopwords_lang))\n",
    "        else:\n",
    "            return set(stopwords.words('english'))\n",
    "    except Exception as e:\n",
    "        print(\"Error in detecting language or loading stopwords:\", e)\n",
    "        return set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_and_split_reviews(reviews):\n",
    "    # Convert reviews to string to avoid TypeError with non-string inputs\n",
    "    if pd.isna(reviews):\n",
    "        return \"\"  # Return an empty string if the review is NaN\n",
    "    reviews = str(reviews)\n",
    "    try:\n",
    "        # Use language-specific stopwords\n",
    "        stop_words = get_stopwords(reviews)\n",
    "    except Exception as e:\n",
    "        print(\"Error using language-specific stopwords:\", e)\n",
    "        stop_words = set(stopwords.words('english'))  # Default to English if error occurs\n",
    "\n",
    "    # Remove all non-alpha characters and extra spaces, convert to lower case\n",
    "    reviews = re.sub('[^\\wáčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ]', ' ', reviews, flags=re.UNICODE)\n",
    "    reviews = re.sub('\\s+', ' ', reviews).strip().lower()\n",
    "    # Remove stopwords\n",
    "    words = [word for word in reviews.split() if word not in stop_words and len(word) > 1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the modified function to your DataFrame\n",
    "df['processed_reviews'] = df['reviews'].apply(preprocess_and_split_reviews)\n",
    "# Apply the modified function to your DataFrame\n",
    "df['processed_reviews'] = df['reviews'].apply(preprocess_and_split_reviews)\n",
    "\n",
    "def get_and_flatten_bigrams(text):\n",
    "    if len(text.split()) < 2:\n",
    "        return []\n",
    "    blob = TextBlob(text)\n",
    "    return [' '.join(bigram) for bigram in blob.ngrams(2)]\n",
    "\n",
    "df['bigrams'] = df['processed_reviews'].apply(get_and_flatten_bigrams)\n",
    "bigrams_df = df.explode('bigrams')[['appId', 'bigrams']].dropna()\n",
    "\n",
    "def flatten_word_frequencies(text):\n",
    "    freqs = Counter(text.split())\n",
    "    return list(freqs.items())\n",
    "\n",
    "df['word_freq'] = df['processed_reviews'].apply(flatten_word_frequencies)\n",
    "word_freq_rows = df.explode('word_freq')\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'appId': word_freq_rows['appId'],\n",
    "    'word': word_freq_rows['word_freq'].apply(lambda x: x[0] if pd.notna(x) else ''),\n",
    "    'frequency': word_freq_rows['word_freq'].apply(lambda x: x[1] if pd.notna(x) else 0)\n",
    "}).dropna()\n",
    "\n",
    "\n",
    "## Install to rating ratio categorization\n",
    "def categorize_install_to_rating_ratio(ratio):\n",
    "    if ratio <= 100:  # Assuming 1 rating per 100 installs or less is high feedback\n",
    "        return 'High Review Ratio'\n",
    "    elif ratio <= 500:  # Assuming between 100 and 500 installs per rating is moderate feedback\n",
    "        return 'Moderate Review Ratio'\n",
    "    else:  # More than 500 installs per rating is considered low feedback\n",
    "        return 'Low Review Ratio'\n",
    "    \n",
    "def compute_sentiment_category_mbert(text):\n",
    "    try:\n",
    "        # Directly pass the text to the pipeline\n",
    "        # The pipeline handles tokenization and truncation internally\n",
    "        result = sentiment_analyzer(text, truncation=True, max_length=512)[0]\n",
    "        label = result['label']\n",
    "        \n",
    "        # Mapping the model output to custom categories\n",
    "        if label == '1 star':\n",
    "            return 'Negative'\n",
    "        elif label == '2 stars':\n",
    "            return 'Slightly negative'\n",
    "        elif label == '3 stars':\n",
    "            return 'Neutral'\n",
    "        elif label == '4 stars':\n",
    "            return 'Slightly positive'\n",
    "        else:  # '5 stars'\n",
    "            return 'Positive'\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return 'Missing'  # Default to 'Missing' in case of an error\n",
    "\n",
    "df['sentiment_category'] = df['reviews'].apply(compute_sentiment_category_mbert)\n",
    "\n",
    "## Rating ratio categorization\n",
    "def categorize_rating_ratio(ratio):\n",
    "    if ratio > 10:\n",
    "        return 'Exceptional'\n",
    "    elif ratio > 5:\n",
    "        return 'Great'\n",
    "    elif ratio > 2:\n",
    "        return 'Good'\n",
    "    elif ratio > 1:\n",
    "        return 'Mixed'\n",
    "    else:\n",
    "        return 'Poor'\n",
    "\n",
    "## App age categorization\n",
    "def categorize_app_age(days):\n",
    "    if days <= 30:\n",
    "        return 'Brand New'\n",
    "    elif days <= 90:\n",
    "        return 'Recently Launched'\n",
    "    elif days <= 365:\n",
    "        return 'Established'\n",
    "    elif days <= 1095:\n",
    "        return 'Mature'\n",
    "    else:\n",
    "        return 'Very Mature'\n",
    "\n",
    "## Price categorization\n",
    "def categorize_price(price):\n",
    "    if price == 0:\n",
    "        return 'Free'\n",
    "    elif price < 1:\n",
    "        return 'Low price'\n",
    "    elif price <= 10:\n",
    "        return 'Medium price'\n",
    "    else:\n",
    "        return 'High price'\n",
    "    \n",
    "\n",
    "## Engagement score categorization\n",
    "def categorize_engagement_score(score, percentiles):\n",
    "    if score >= percentiles[0.9]:\n",
    "        return 'Very High Engagement'\n",
    "    elif score >= percentiles[0.75]:\n",
    "        return 'High Engagement'\n",
    "    elif score >= percentiles[0.5]:\n",
    "        return 'Moderate Engagement'\n",
    "    elif score >= percentiles[0.25]:\n",
    "        return 'Low Engagement'\n",
    "    else:\n",
    "        return 'Very Low Engagement'\n",
    "\n",
    "## Update frequency categorization\n",
    "def categorize_update_frequency(days_since_last_update):\n",
    "    if days_since_last_update <= 30:\n",
    "        return 'Very Recent Updates'\n",
    "    elif days_since_last_update <= 90:\n",
    "        return 'Recently Updated'\n",
    "    elif days_since_last_update <= 180:\n",
    "        return 'Moderately Updated'\n",
    "    elif days_since_last_update <= 365:\n",
    "        return 'Rarely Updated'\n",
    "    else:\n",
    "        return 'Stale'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Category parsing\n",
    "def parse_json_categories(row):\n",
    "    try:\n",
    "        categories_list = json.loads(row)\n",
    "        # Extract just the names from each category\n",
    "        return [category['name'] for category in categories_list]\n",
    "    except:\n",
    "        return []  # Return an empty list if parsing fails or if row is empty\n",
    "\n",
    "df['categories'] = df['categories'].apply(parse_json_categories)\n",
    "\n",
    "\n",
    "categories_exploded = df[['appId', 'categories']].explode('categories')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to convert binary values to 'free' or 'paid'\n",
    "def free_convert_to_category(value):\n",
    "    if value == 1:\n",
    "        return 'free'\n",
    "    else:\n",
    "        return 'paid'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Histogram parsing\n",
    "def parse_histogram(row):\n",
    "    try:\n",
    "        histogram_dict = json.loads(row)\n",
    "    except json.JSONDecodeError:\n",
    "        return pd.Series([float('nan')] * 5)\n",
    "    return pd.Series(histogram_dict)\n",
    "histogram_columns = df['histogram'].apply(parse_histogram)\n",
    "histogram_columns.columns = ['1*', '2*', '3*', '4*', '5*']\n",
    "df = pd.concat([df, histogram_columns], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#Calculations \n",
    "df['app_age'] = (df['updated'] - df['released']).dt.days\n",
    "df['rating_ratio'] = (df['4*'] + df['5*']) / (df['1*'] + df['2*'])\n",
    "df['engagement_score'] = (df['score'] * df['ratings']) / df['minInstalls']\n",
    "df['install_to_rating'] = df['minInstalls'] / (df['ratings'] + 1e-10)\n",
    "\n",
    "\n",
    "\n",
    "# Load your DataFrame (assuming you've already loaded it into 'df')\n",
    "df['IAPRange'] = df['IAPRange'].astype(str)  # Ensure the column is treated as string\n",
    "\n",
    "# Extract using the updated regex\n",
    "df[['CurrencySymbolMin', 'IAPMin', 'CurrencySymbolMax', 'IAPMax']] = df['IAPRange'].str.extract(r'([^\\d]+)(\\d+[\\.,]?\\d*) - ([^\\d]+)(\\d+[\\.,]?\\d*)')\n",
    "\n",
    "# Normalize decimal points and convert to float\n",
    "df['IAPMin'] = df['IAPMin'].str.replace(',', '.').astype(float)\n",
    "df['IAPMax'] = df['IAPMax'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Optionally clean up currency symbols by stripping spaces or other characters\n",
    "df['CurrencySymbolMin'] = df['CurrencySymbolMin'].str.strip()\n",
    "df['CurrencySymbolMax'] = df['CurrencySymbolMax'].str.strip()\n",
    "\n",
    "# Preview the results\n",
    "\n",
    "## Categorizations\n",
    "df['update_frequency'] = df['days_since_last_update'].apply(categorize_update_frequency)\n",
    "df['app_age_category'] = df['app_age'].apply(lambda days: categorize_app_age(days))\n",
    "df['rating_ratio_category'] = df['rating_ratio'].apply(lambda ratio: categorize_rating_ratio(ratio))\n",
    "percentiles = df['engagement_score'].quantile([0.25, 0.5, 0.75, 0.9]).to_dict()\n",
    "df['engagement_score_category'] = df['engagement_score'].apply(lambda x: categorize_engagement_score(x, percentiles))\n",
    "df['price_category'] = df['price'].apply(lambda price: categorize_price(price))\n",
    "df['install_to_rating_category'] = df['install_to_rating'].apply(categorize_install_to_rating_ratio)\n",
    "df['free'] = df['free'].apply(free_convert_to_category)\n",
    "\n",
    "# Clean-up and Output\n",
    "columns_to_remove = [\n",
    "    'description', 'descriptionHTML', 'summary', 'installs', 'maxInstalls', 'scoreText', 'reviews', 'histogram', 'currency', 'androidVersion', 'androidVersionText',\n",
    "    'androidMaxVersion', 'previewVideo', 'developerId', 'developerEmail', 'developerAddress', 'privacyPolicy', 'developerInternalID', 'genreId', 'icon', 'headerImage',\n",
    "    'screenshots', 'video', 'videoImage','contentRatingDescription','version', 'recentChanges', 'comments', 'originalPrice', 'discountEndDate', 'categories',  'priceText',\n",
    "    '1*', '2*', '3*', '4*', '5*', 'processed_reviews', 'bigrams_df', 'word_freq_df'\n",
    "]\n",
    "df.drop(columns_to_remove, axis=1, inplace=True, errors='ignore')\n",
    "df['updated'] = df['updated'].dt.strftime('%Y-%m-%d')\n",
    "df['released'] = df['released'].dt.strftime('%Y-%m-%d')\n",
    "df.to_csv('GooglePlayOutput_cleaned.csv', index=False, sep=',', encoding='utf-8')\n",
    "categories_exploded.to_csv('GooglePlay_Categories.csv', index=False)\n",
    "bigrams_df.to_csv('GooglePlay_Bigrams.csv', index=False)\n",
    "word_freq_df.to_csv('GooglePlay_Word_Frequencies.csv', index=False)\n",
    "\n",
    "print(df.head())  # This will print the first 5 rows of the DataFrame after cleanup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
